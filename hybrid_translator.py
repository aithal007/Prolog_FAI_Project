#!/usr/bin/env python3
"""
Hybrid AI + Prolog translator server
Serves a static frontend and provides /api/convert which accepts JSON {"sentence":"..."}
and returns predicate logic generated by Prolog using DCG rules.

Requires: flask, flask-cors, spacy, pyswip (optional)
Install: pip install -r requirements.txt
Download spaCy model: python -m spacy download en_core_web_sm
Start: python hybrid_translator.py
"""
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import os
import traceback

APP_DIR = os.path.dirname(os.path.abspath(__file__))
STATIC_DIR = os.path.join(APP_DIR, 'static')

app = Flask(__name__, static_folder=STATIC_DIR, static_url_path='')
CORS(app)

try:
    import spacy
    nlp = spacy.load('en_core_web_sm')
except Exception as e:
    nlp = None
    print('spaCy not available or model not loaded:', e)

# Try pyswip; if not available, we'll fallback to calling swipl via subprocess
USE_PYSWIP = True
try:
    from pyswip import Prolog
    prolog = Prolog()
    # consult the Prolog KB if present
    kb = os.path.join(APP_DIR, 'logic_translator.pl')
    if os.path.exists(kb):
        try:
            prolog.consult(kb)
            print('Consulted Prolog KB:', kb)
        except Exception as e:
            print('pyswip consult failed:', e)
    else:
        print('Prolog KB not found at', kb)
except Exception as e:
    print('pyswip not available, will fallback to calling swipl:', e)
    USE_PYSWIP = False

import shlex
import subprocess


def tokens_from_sentence(sentence):
    """Return a list of dicts [{'text':..., 'pos':...}] representing tokens.
    If spaCy is available, use it to lemmatize verbs/nouns; otherwise simple split.
    """
    result = []
    if nlp:
        doc = nlp(sentence.lower())
        dets = set(['every', 'all', 'each', 'any', 'a', 'an', 'some', 'one'])
        for i, token in enumerate(doc):
            if token.is_punct or token.is_space:
                continue
            pos = token.pos_
            text = token.text.lower()
            if pos == 'DET' and text in dets:
                result.append({'text': text, 'pos': 'DET'})
                continue
            if pos == 'VERB':
                lemma = token.lemma_.lower() if token.lemma_ else text
                result.append({'text': lemma, 'pos': 'VERB'})
                continue
            if pos in ('NOUN', 'PROPN'):
                lemma = token.lemma_.lower() if token.lemma_ else text
                result.append({'text': lemma, 'pos': 'NOUN'})
                continue
            if pos == 'ADJ':
                # skip adjectives that directly precede a noun (we keep the noun)
                if i + 1 < len(doc) and doc[i+1].pos_ in ('NOUN', 'PROPN'):
                    continue
                result.append({'text': token.lemma_.lower() if token.lemma_ else text, 'pos': 'ADJ'})
                continue
            # fallback
            result.append({'text': token.lemma_.lower() if token.lemma_ else text, 'pos': pos})
        return result
    # fallback simple tokenizer
    cleaned = ''.join(ch if ch.isalnum() or ch.isspace() else ' ' for ch in sentence.lower())
    for w in cleaned.split():
        result.append({'text': w, 'pos': 'UNK'})
    return result
    # fallback
    cleaned = ''.join(ch if ch.isalnum() or ch.isspace() else ' ' for ch in sentence.lower())
    return [w for w in cleaned.split() if w]


def prolog_list_from_tokens(tokens_with_pos):
    # Accept list of dicts or simple strings. Return Prolog list string [every,student,read,a,book]
    if not tokens_with_pos:
        return '[]'
    if isinstance(tokens_with_pos[0], dict):
        toks = [t['text'] for t in tokens_with_pos]
    else:
        toks = tokens_with_pos
    return '[' + ','.join(toks) + ']'


def load_kb_terms():
    """Parse logic_translator.pl to extract known noun and verb tokens."""
    kb_path = os.path.join(APP_DIR, 'logic_translator.pl')
    nouns = set()
    verbs = set()
    if not os.path.exists(kb_path):
        return nouns, verbs
    import re
    text = open(kb_path, 'r', encoding='utf-8').read()
    for m in re.finditer(r"noun\(\s*([a-zA-Z0-9_]+)\s*,", text):
        nouns.add(m.group(1))
    for m in re.finditer(r"verb\(\s*([a-zA-Z0-9_]+)\s*,", text):
        verbs.add(m.group(1))
    return nouns, verbs


def query_prolog_phrase(tokens):
    prolog_list = prolog_list_from_tokens(tokens)
    query = f"phrase(sentence(Logic), {prolog_list})"
    if USE_PYSWIP:
        try:
            res = list(prolog.query(query))
            if res:
                logic = res[0].get('Logic')
                return True, str(logic)
            # No result from pyswip: try to auto-assert missing nouns/verbs into the running Prolog KB
            try:
                kb_nouns, kb_verbs = load_kb_terms()
                missing_nouns = set()
                missing_verbs = set()
                for t in tokens:
                    if isinstance(t, dict):
                        txt = t['text']
                        pos = t['pos']
                        if pos == 'NOUN' and txt not in kb_nouns:
                            missing_nouns.add(txt)
                        if pos == 'VERB' and txt not in kb_verbs:
                            missing_verbs.add(txt)
                # Assert into Prolog using pyswip
                for n in sorted(missing_nouns):
                    try:
                        prolog.assertz(f"noun({n}, X, {n}(X)) --> [{n}].")
                    except Exception as e:
                        msg = str(e)
                        # If a previous query wasn't closed, re-create Prolog instance and re-consult KB
                        if 'last query was not closed' in msg.lower() or 'not closed' in msg.lower():
                            try:
                                print('Reinitializing Prolog engine due to open query...')
                                global prolog
                                prolog = Prolog()
                                if os.path.exists(kb):
                                    prolog.consult(kb)
                                prolog.assertz(f"noun({n}, X, {n}(X)) --> [{n}].")
                            except Exception:
                                pass
                for v in sorted(missing_verbs):
                    try:
                        prolog.assertz(f"verb({v}, S, O, {v}(S,O)) --> [{v}].")
                    except Exception as e:
                        msg = str(e)
                        if 'last query was not closed' in msg.lower() or 'not closed' in msg.lower():
                            try:
                                print('Reinitializing Prolog engine due to open query...')
                                global prolog
                                prolog = Prolog()
                                if os.path.exists(kb):
                                    prolog.consult(kb)
                                prolog.assertz(f"verb({v}, S, O, {v}(S,O)) --> [{v}].")
                            except Exception:
                                pass
                # Retry query
                res2 = list(prolog.query(query))
                if res2:
                    logic = res2[0].get('Logic')
                    return True, str(logic)
            except Exception:
                pass
            return False, 'No parse'
        except Exception as e:
            return False, f'pyswip query error: {e}'
    # fallback: call swipl and capture output
    try:
        # build a small prolog command that consults the KB and runs the phrase/2, then writes term
        kb = os.path.join(APP_DIR, 'logic_translator.pl')
        swipl_cmd = [
            'swipl', '-q', '-s', kb,
            '-g', f"(phrase(sentence(Logic), {prolog_list}) -> write(Logic) ; write('NO_PARSE')), halt"
        ]
        proc = subprocess.run(swipl_cmd, capture_output=True, text=True, check=False)
        out = proc.stdout.strip()
        if out and out != 'NO_PARSE':
            return True, out
        # If parse failed, try to auto-add missing nouns/verbs and retry with temp KB
        # Determine missing terms by comparing tokens to KB
        try:
            kb_nouns, kb_verbs = load_kb_terms()
            tokens_list = [t['text'] if isinstance(t, dict) else t for t in tokens]
            # decide pos from tokens if dict present
            missing_nouns = set()
            missing_verbs = set()
            for t in tokens:
                if isinstance(t, dict):
                    txt = t['text']
                    pos = t['pos']
                    if pos == 'NOUN' and txt not in kb_nouns:
                        missing_nouns.add(txt)
                    if pos == 'VERB' and txt not in kb_verbs:
                        missing_verbs.add(txt)
            if missing_nouns or missing_verbs:
                # create temporary KB file that consults the original and adds DCG entries
                import tempfile
                tmp_fd, tmp_path = tempfile.mkstemp(suffix='.pl', prefix='tmp_kb_', dir=APP_DIR)
                with os.fdopen(tmp_fd, 'w', encoding='utf-8') as f:
                    f.write(f":- consult('{os.path.join(APP_DIR,'logic_translator.pl').replace('\\','/')}').\n")
                    for n in sorted(missing_nouns):
                        f.write(f"noun({n}, X, {n}(X)) --> [{n}].\n")
                    for v in sorted(missing_verbs):
                        f.write(f"verb({v}, S, O, {v}(S,O)) --> [{v}].\n")
                swipl_cmd2 = ['swipl', '-q', '-s', tmp_path,
                             '-g', f"(phrase(sentence(Logic), {prolog_list}) -> write(Logic) ; write('NO_PARSE')), halt"]
                proc2 = subprocess.run(swipl_cmd2, capture_output=True, text=True, check=False)
                out2 = proc2.stdout.strip()
                try:
                    os.remove(tmp_path)
                except Exception:
                    pass
                if out2 and out2 != 'NO_PARSE':
                    return True, out2
        except Exception:
            pass
        return False, 'No parse (swipl)'
    except Exception as e:
        return False, f'swipl call failed: {e}'


@app.route('/')
def index():
    return send_from_directory(STATIC_DIR, 'index.html')


@app.route('/api/convert', methods=['POST'])
def api_convert():
    try:
        data = request.get_json(force=True)
        sentence = data.get('sentence', '')
        if not sentence:
            return jsonify({'success': False, 'error': 'No sentence provided'}), 400

        tokens = tokens_from_sentence(sentence)
        success, result = query_prolog_phrase(tokens)

        resp = {
            'success': success,
            'sentence': sentence,
            'tokens': tokens,
            'logic': result if success else None,
            'error': None if success else result
        }
        return jsonify(resp)
    except Exception as e:
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/static/<path:path>')
def static_proxy(path):
    return send_from_directory(STATIC_DIR, path)


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8000))
    print(f'Starting Flask server on http://localhost:{port}')
    app.run(host='0.0.0.0', port=port, debug=True)
