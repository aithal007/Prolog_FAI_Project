#!/usr/bin/env python3
"""
Hybrid AI + Prolog translator server
Serves a static frontend and provides /api/convert which accepts JSON {"sentence":"..."}
and returns predicate logic generated by Prolog using DCG rules.

Requires: flask, flask-cors, spacy, pyswip (optional)
Install: pip install -r requirements.txt
Download spaCy model: python -m spacy download en_core_web_sm
Start: python hybrid_translator.py
"""
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import os
import traceback

APP_DIR = os.path.dirname(os.path.abspath(__file__))
STATIC_DIR = os.path.join(APP_DIR, 'static')

app = Flask(__name__, static_folder=STATIC_DIR, static_url_path='')
CORS(app)

try:
    import spacy
    nlp = spacy.load('en_core_web_sm')
except Exception as e:
    nlp = None
    print('spaCy not available or model not loaded:', e)

# Try pyswip; if not available, we'll fallback to calling swipl via subprocess
USE_PYSWIP = True
try:
    from pyswip import Prolog
    prolog = Prolog()
    # consult the Prolog KB if present
    kb = os.path.join(APP_DIR, 'logic_translator.pl')
    if os.path.exists(kb):
        try:
            prolog.consult(kb)
            print('Consulted Prolog KB:', kb)
        except Exception as e:
            print('pyswip consult failed:', e)
    else:
        print('Prolog KB not found at', kb)
except Exception as e:
    print('pyswip not available, will fallback to calling swipl:', e)
    USE_PYSWIP = False

import shlex
import subprocess


def tokens_from_sentence(sentence):
    """Return a list of lower-case tokens suitable for Prolog DCG.
    If spaCy is available, use it to lemmatize verbs/nouns; otherwise simple split.
    """
    if nlp:
        doc = nlp(sentence.lower())
        toks = []
        dets = set(['every', 'all', 'each', 'any', 'a', 'an', 'some', 'one'])
        for i, token in enumerate(doc):
            if token.is_punct or token.is_space:
                continue
            # Keep determiners (quantifiers) as-is
            if token.pos_ == 'DET' and token.text.lower() in dets:
                toks.append(token.text.lower())
                continue
            # Verbs: use lemma
            if token.pos_ == 'VERB':
                lemma = token.lemma_.lower() if token.lemma_ else token.text.lower()
                toks.append(lemma)
                continue
            # Nouns / proper nouns: use lemma (prefer head noun, skip compounds/adjectives)
            if token.pos_ in ('NOUN', 'PROPN'):
                # prefer the syntactic head if this token is part of a compound, choose the noun itself
                lemma = token.lemma_.lower() if token.lemma_ else token.text.lower()
                toks.append(lemma)
                continue
            # Adjectives often modify nouns (e.g., 'mobile phone'); skip them so head noun remains
            if token.pos_ == 'ADJ':
                # only keep adjectives when not followed by a noun (rare for our domain)
                if i + 1 < len(doc) and doc[i+1].pos_ in ('NOUN','PROPN'):
                    continue
                else:
                    toks.append(token.lemma_.lower() if token.lemma_ else token.text.lower())
                    continue
            # Fallback: include lemmas for other content words
            if token.pos_ in ('PRON', 'NUM'):
                toks.append(token.lemma_.lower() if token.lemma_ else token.text.lower())
                continue
        return toks
    # fallback
    cleaned = ''.join(ch if ch.isalnum() or ch.isspace() else ' ' for ch in sentence.lower())
    return [w for w in cleaned.split() if w]


def prolog_list_from_tokens(tokens):
    # Build Prolog atom list: [every, student, read, a, book]
    return '[' + ','.join(tokens) + ']'


def query_prolog_phrase(tokens):
    prolog_list = prolog_list_from_tokens(tokens)
    query = f"phrase(sentence(Logic), {prolog_list})"
    if USE_PYSWIP:
        try:
            res = list(prolog.query(query))
            if res:
                logic = res[0].get('Logic')
                return True, str(logic)
            return False, 'No parse'
        except Exception as e:
            return False, f'pyswip query error: {e}'
    # fallback: call swipl and capture output
    try:
        # build a small prolog command that consults the KB and runs the phrase/2, then writes term
        kb = os.path.join(APP_DIR, 'logic_translator.pl')
        swipl_cmd = [
            'swipl', '-q', '-s', kb,
            '-g', f"(phrase(sentence(Logic), {prolog_list}) -> write(Logic) ; write('NO_PARSE')), halt"
        ]
        proc = subprocess.run(swipl_cmd, capture_output=True, text=True, check=False)
        out = proc.stdout.strip()
        if out and out != 'NO_PARSE':
            return True, out
        return False, 'No parse (swipl)'
    except Exception as e:
        return False, f'swipl call failed: {e}'


@app.route('/')
def index():
    return send_from_directory(STATIC_DIR, 'index.html')


@app.route('/api/convert', methods=['POST'])
def api_convert():
    try:
        data = request.get_json(force=True)
        sentence = data.get('sentence', '')
        if not sentence:
            return jsonify({'success': False, 'error': 'No sentence provided'}), 400

        tokens = tokens_from_sentence(sentence)
        success, result = query_prolog_phrase(tokens)

        resp = {
            'success': success,
            'sentence': sentence,
            'tokens': tokens,
            'logic': result if success else None,
            'error': None if success else result
        }
        return jsonify(resp)
    except Exception as e:
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/static/<path:path>')
def static_proxy(path):
    return send_from_directory(STATIC_DIR, path)


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8000))
    print(f'Starting Flask server on http://localhost:{port}')
    app.run(host='0.0.0.0', port=port, debug=True)
